# by Jason Bryer

# Model 1: All assessments
# Model 2: SRL, reading, math
# Model 3: SRL
# Model 4: No DAACS info

# A function that takes a formula and a training dataset and returns a list of models. It creates a list of models, models, and reports the number of missing rows. Then it populates the list of models. Extends all the way past the models$ section.
train_models_lr <- function(formu, daacs.train) {
	models <- list()
	
	tmp1 <- daacs.train %>%
		filter(!is.na(srlTotal) & !is.na(writeTotal) & !is.na(readTotal) & !is.na(mathTotal))
	tmp2 <- daacs.train %>%
		filter(!DAACS_ID %in% tmp1$DAACS_ID & !is.na(srlTotal) & !is.na(readTotal) & !is.na(mathTotal))
	tmp3 <- daacs.train %>%
		filter(!DAACS_ID %in% tmp1$DAACS_ID & !DAACS_ID %in% tmp2$DAACS_ID & !is.na(srlTotal))
	tmp4 <- daacs.train %>%
		filter(!DAACS_ID %in% tmp1$DAACS_ID & !DAACS_ID %in% tmp2$DAACS_ID & !DAACS_ID %in% tmp3$DAACS_ID)
	
	missing <- sum(!daacs.train$DAACS_ID %in% c(tmp1$DAACS_ID, tmp2$DAACS_ID, tmp3$DAACS_ID, tmp4$DAACS_ID)) > 0
	if(missing > 0) {
		stop(paste0(missing, ' rows not assigned to model!'))
	}

######
# These are the models that are getting placed into the list of models generated by train_models_lr. lr stands for logistic regression. 

	  # Uses the syntax of update.formula to replace the formula, formu, with another one. This new formula is based on what JB found to be predictive for students with a particular subset of DAACS assessments completed. See comments Model x:... at top. Also the model specified uses the daacs.train data with the appropriate na rows removed.
	
		models$model1 <- glm(update.formula(formu,  ~ . + 
											srl_strategies + srl_metacognition + 
											srl_anxiety + srl_mastery_orientation + srl_mindset + srl_self_efficacy + 
											writeTotal + readTotal + mathTotal), 
						 data = tmp1,
						 family = binomial(link = 'logit'))
	models$model2 <- glm(update.formula(formu,  ~ . + 
											srl_strategies + srl_metacognition + 
											srl_anxiety + srl_mastery_orientation + srl_mindset + srl_self_efficacy +
											readTotal + mathTotal), 
						 data = tmp2,
						 family = binomial(link = 'logit'))
	models$model3 <- glm(update.formula(formu,  ~ . + 
											srl_strategies + srl_metacognition + 
											srl_anxiety + srl_mastery_orientation + srl_mindset + srl_self_efficacy), 
						 data = tmp3,
						 family = binomial(link = 'logit'))
	models$model4 <- glm(update.formula(formu,  ~ .), 
						 data = tmp4,
						 family = binomial(link = 'logit'))
	
	return(models)
}

#This function does something analogous with random forest models. 

train_models_rf <- function(formu, daacs.train, seed = 2112) {
	if(!is.null(seed)) { set.seed(seed) }
	
	models <- list()
	
	tmp1 <- daacs.train %>%
		filter(!is.na(srlTotal) & !is.na(writeTotal) & !is.na(readTotal) & !is.na(mathTotal))
	tmp2 <- daacs.train %>%
		filter(!DAACS_ID %in% tmp1$DAACS_ID & !is.na(srlTotal) & !is.na(readTotal) & !is.na(mathTotal))
	tmp3 <- daacs.train %>%
		filter(!DAACS_ID %in% tmp1$DAACS_ID & !DAACS_ID %in% tmp2$DAACS_ID & !is.na(srlTotal))
	tmp4 <- daacs.train %>%
		filter(!DAACS_ID %in% tmp1$DAACS_ID & !DAACS_ID %in% tmp2$DAACS_ID & !DAACS_ID %in% tmp3$DAACS_ID)
	
	missing <- sum(!daacs.train$DAACS_ID %in% c(tmp1$DAACS_ID, tmp2$DAACS_ID, tmp3$DAACS_ID, tmp4$DAACS_ID)) > 0
	if(missing > 0) {
		stop(paste0(missing, ' rows not assigned to model!'))
	}
	
	models$model1 <- randomForest(update.formula(formu,  ~ . + 
											srl_strategies + srl_metacognition + 
											srl_anxiety + srl_mastery_orientation + srl_mindset + srl_self_efficacy + 
											writeTotal + readTotal + mathTotal), 
						 data = tmp1)
	models$model2 <- randomForest(update.formula(formu,  ~ . + 
											srl_strategies + srl_metacognition + 
											srl_anxiety + srl_mastery_orientation + srl_mindset + srl_self_efficacy +
											readTotal + mathTotal), 
						 data = tmp2)
	models$model3 <- randomForest(update.formula(formu,  ~ . + 
											srl_strategies + srl_metacognition + 
											srl_anxiety + srl_mastery_orientation + srl_mindset + srl_self_efficacy), 
						 data = tmp3)
	models$model4 <- randomForest(update.formula(formu,  ~ .), 
						 data = tmp4)
	
	return(models)
}




get_predictions <- function(models, df) {

#These tmps are dataframes, subsets of the dataframe passed to the function.  
	tmp1 <- !is.na(df$srlTotal) & 
		!is.na(df$writeTotal) & 
		!is.na(df$readTotal) &
		!is.na(df$mathTotal)
	tmp2 <- !tmp1 & 
		!is.na(df$srlTotal) & 
		!is.na(df$readTotal) & 
		!is.na(df$mathTotal)
	tmp3 <- !tmp1 & !tmp2 & 
		!is.na(df$srlTotal)
	tmp4 <- !tmp1 & !tmp2 & !tmp3
	
	#What is this section doing? Adding columns to the df for the prediction, the model used, and the "Prediction_Class," which I don't understand. Then I think it's populating each column with NAs and also specifying the data type. 
	
	df$Prediction <- NA_real_
	df$Model <- NA_character_
	df$Prediction_Class <- NA_integer_
	
	#Here the dataframe gets populated with predictions from each model, and a roc curve is constructed? I don't understand what happens after roc1. (?)
	
	df[tmp1,]$Prediction <- predict(models$model1, newdata = df[tmp1,], type = "response")
	if('glm' %in% class(models$model1)) {
		roc1 <- calculate_roc(predict(models$model1, newdata = models$model1$data, type = "response"),
							  models$model1$data[,all.vars(models$model1$formula)[1], drop = TRUE])
		df[tmp1,]$Prediction_Class <- df[tmp1,]$Prediction > attr(roc1, 'threshold')
	} else {
		df[tmp1,]$Prediction_Class <- df[tmp1,]$Prediction
	}
	df[tmp1,]$Model <- 'Model1'
	
	df[tmp2,]$Prediction <- predict(models$model2, newdata = df[tmp2,], type = "response")
	if('glm' %in% class(models$model2)) {
		roc2 <- calculate_roc(predict(models$model2, newdata = models$model2$data, type = "response"),
							  models$model2$data[,all.vars(models$model2$formula)[1], drop = TRUE])
		df[tmp2,]$Prediction_Class <- df[tmp2,]$Prediction > attr(roc2, 'threshold')
	} else {
		df[tmp2,]$Prediction_Class <- df[tmp2,]$Prediction
	}
	df[tmp2,]$Model <- 'Model2'
	
	df[tmp3,]$Prediction <- predict(models$model3, newdata = df[tmp3,], type = "response")
	if('glm' %in% class(models$model3)) {
		roc3 <- calculate_roc(predict(models$model3, newdata = models$model3$data, type = "response"),
							  models$model3$data[,all.vars(models$model3$formula)[1], drop = TRUE])
		df[tmp3,]$Prediction_Class <- df[tmp3,]$Prediction > attr(roc3, 'threshold')
	} else {
		df[tmp3,]$Prediction_Class <- df[tmp3,]$Prediction
	}
	df[tmp3,]$Model <- 'Model3'
	
	df[tmp4,]$Prediction <- predict(models$model4, newdata = df[tmp4,], type = "response")
	if('glm' %in% class(models$model4)) {
		roc4 <- calculate_roc(predict(models$model4, newdata = models$model4$data, type = "response"),
							  models$model4$data[,all.vars(models$model4$formula)[1], drop = TRUE])
		df[tmp4,]$Prediction_Class <- df[tmp4,]$Prediction > attr(roc4, 'threshold')
	} else {
		df[tmp4,]$Prediction_Class <- df[tmp4,]$Prediction
	}
	df[tmp4,]$Model <- 'Model4'
	
	return(df)
}


#Below, train_fun is not a function. It's the type of training function, either train_models_lr, or train_models_rf.

#' 
#' 
#' @param ... other parameters passed to train_fun
run_prediction_analysis_ec <- function(formu, train, valid, train_fun = train_models_lr, ...) {
	results <- list()
	results$formu <- formu
	results$train <- train
	results$valid <- valid
	results$models <- train_fun(formu, train, ...)
	results$predictions <- get_predictions(results$models, valid)
	y <- valid[,all.vars(formu)[1]]
	if(sum(is.na(y)) > 0) {
		warning('Missing predictions')
	}
	results$base_success <- table(y) %>% prop.table() %>% max()
	results$confusion_matrix <- table(valid[,all.vars(formu)[1]], 
									  results$predictions$Prediction_Class,
									  useNA='ifany')
	results$confusion_matrix_percent <- prop.table(results$confusion_matrix)
	results$prediction_accuracy <- (results$confusion_matrix_percent[1,1] + 
										results$confusion_matrix_percent[2,2])
	results$difference <- results$prediction_accuracy - results$base_success
	cat(paste0("Prediction ", ifelse(results$difference > 0, " increased ", ' decreased '),
			   "by ", round(results$difference * 100, digits = 2), "%"))
	return(results)
}

#' Run prediction models with logistic regression
run_prediction_analysis_lr <- function(formu, train, valid, ...) {
	results <- list()
	results$formu <- formu
	results$train <- train
	results$valid <- valid
	results$model <- glm(formu, 
						 data = train, 
						 family = binomial(link = 'logit'))
	results$valid$Prediction <- predict(results$model, newdata = valid)
	
	results$roc <- calculate_roc(
		predictions = predict(results$model, 
							  newdata = train,
							  type = 'response'),
		survived = train[, all.vars(formu)[1], drop = TRUE])
	results$valid$Prediction_Class <- results$valid$Prediction > attr(results$roc, 'threshold')
	
	y <- valid[,all.vars(formu)[1]]
	if(sum(is.na(y)) > 0) {
		warning('Missing predictions')
	}
	results$base_success <- table(y) %>% prop.table() %>% max()
	results$confusion_matrix <- table(y, 
									  results$valid$Prediction_Class,
									  useNA='ifany')
	results$confusion_matrix_percent <- prop.table(results$confusion_matrix)
	results$prediction_accuracy <- (results$confusion_matrix_percent[1,1] + 
										results$confusion_matrix_percent[2,2])
	results$difference <- results$prediction_accuracy - results$base_success
	cat(paste0("Prediction ", ifelse(results$difference > 0, " increased ", ' decreased '),
			   "by ", round(results$difference * 100, digits = 2), "%"))
	return(results)
}

# Run prediction models with random forests
run_prediction_analysis_rf <- function(formu, train, valid, seed = 2112, ...) {
	if(!is.null(seed)) { set.seed(seed) }
	
	results <- list()
	results$formu <- formu
	results$train <- train
	results$valid <- valid
	results$model <- randomForest(formu, data = train)
	results$valid$Prediction <- predict(results$model, newdata = valid, type = 'response')
	y <- valid[,all.vars(formu)[1]]
	if(sum(is.na(y)) > 0) {
		warning('Missing predictions')
	}
	results$base_success <- table(y) %>% prop.table() %>% max()
	results$confusion_matrix <- table(y, 
									  results$valid$Prediction,
									  useNA='ifany')
	results$confusion_matrix_percent <- prop.table(results$confusion_matrix)
	results$prediction_accuracy <- (results$confusion_matrix_percent[1,1] + 
										results$confusion_matrix_percent[2,2])
	results$difference <- results$prediction_accuracy - results$base_success
	cat(paste0("Prediction ", ifelse(results$difference > 0, " increased ", ' decreased '),
			   "by ", round(results$difference * 100, digits = 2), "%"))
	return(results)
}

